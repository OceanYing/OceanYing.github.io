<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis">
    <meta name="author" content="Haiyang Ying,
                                Baowei Jiang,
                                Jinzhi Zhang,
                                Di Xu,
                                Tao Yu,
                                Qionghai Dai,
                                Lu Fang">

    <title>PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis</h2>
    <h3>ICCV 2023</h3>
    <hr>
    <p class="authors">
        <a href="https://oceanying.github.io/" target="_blank"> Haiyang Ying</a>,
        <a target="_blank"> Baowei Jiang</a>,
        <a target="_blank"> Jinzhi Zhang</a>,
        <a target="_blank"> Di Xu</a>,
        <a href="https://ytrock.com/" target="_blank"> Tao Yu</a>,
        <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html" target="_blank"> Qionghai Dai</a>,
        <a href="http://luvision.net/" target="_blank"> Lu Fang</a>,
    </p>

    <p class="authors">
       <small> Tsinghua University </small>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2103.15875" target="_blank"> Paper (Arxiv) </a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://youtu.be/FpShWO7LVbM" target="_blank"> Video (Youtube) </a>
        <!-- <a class="btn btn-primary" href="https://www.bilibili.com/video/BV1FK4y1M7PC/"> Video (bilibili) </a> -->
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/THU-luvision/PARF" target="_blank"> Code (Coming Soon) </a>
    </div>
</div>

<div class="container">
    <div class="section">
        <img src="img/Teaser.png" title="Performance comparison with the state-of-the-art radiance field reconstruction methods" class="center" width="100%">
<!--         <embed src="img/Teaser_v4.pdf" width="500" height="375" type="application/pdf"> -->
        <hr>
        <p>
            We present PARF, Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis, which enables both efficient and high-quality rendering in general indoor scenes. 
            Standard volume based rendering methods like NeRF can model complex scenes but suffer from heavy sampling and ambiguous geometry.
            Primitive based rendering methods like NeurMips enjoys fast rendering but have difficulty representing complex geometric.
            </br>
            </br>
        </p>
    </div>

    <div class="section">
        <h2>Representation</h2>
        <hr>
        <img src="img/figure2_final.png" title="Representation" class="center" width="80%">
        <hr>
        <p>
            We propose a novel hybrid representation to take advantage of both kinds of methods.
            Specifically, we represent the scene with a semantic volume, which consists three kinds of voxels with different sampling strategy. 
            D-voxels we simply use dense sampling like nerf, while for P-voxels we apply sparse primitive aware sampling strategy. 
            Besides, we simply skip sampling within E-voxels.
        </p>
    </div>
    
    <div class="section">
        <h2>Framework</h2>
        <hr>
        <img src="img/figure3_final.png" title="Framework" class="center" width="80%">
        <hr>
        <p>
            we show the optimization framework for incremental radiance reconstruction.
            Given each RGBD image as input, we first detect primitives such as planes.
            Then we merge the detected new primitives into a global primitive list.
            Next, we back project the semantic frame into a 3D semantic volume to update the global semantic state.
            After that, primitive aware hybrid rendering is applied to render RGB values, depth values as well as semantic values which are supervised by the input frames.
        </p>
    </div>

    <div class="section">
        <h2>Incremental Performance</h2>
        <hr>
        <p>
            We provide performance comparison between PARF and NeRF-SLAM which is in depth supervised version of InstantNGP.
            Notice our PARF enjoys much faster convergence with the help of primitive-aware hybrid representation.
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-10">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-10">
                <h5>Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-10">
                <h5>Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Extropolation Performance</h2>
        <hr>
        <p>
            We also show the extrapolation ability of our method. 
            Note that under extrapolation views, PARF shows robust rendering results with the help of primitive-aware representation, 
            while NeRF-SLAM shows blurry rendering results due to the ambiguous geometry.
        </p>
        
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        </br>
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>BundleFusion apt0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra4.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>

        
    <div class="section">
        <h2>Real-time Interaction and Rendering</h2>
        <hr>
        <p>
            We show that our method is capable of real-time rendering and interactions.
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-12">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-12">
                <h5>Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Sparse Reconstruction</h2>
        <p>
            Given only sparse view as input, PARF shows robust rendering performance thanks to the primitive based hybrid representation.
        </p>
        <hr>
        <img src="img/sparsity.png" title="Representation" class="center" width="80%">
    </div>

    
    <div class="section">
        <h2>Scene Editing</h2>
        <p>
            Our primitive-aware hybrid representation also enables convenient scene editing.
        </p>
        <hr>
        <img src="img/Editing.png" title="Representation" class="center" width="80%">
    </div>

    

    <div class="section">
        <h2>Semantic Label Super-Resolution</h2>
        <hr>
        <p>
            Low-resolution labelling from light-weight CNNs or manual annotations are less costly to acquire than high-resolution ones. 
            By supervising Semantic-NeRF with only low-resolution labels, we can accurately super-resolve input labels.

            <br>
            Note that either coarse or sparse training labels use the same amount of information from low-resolution label maps.
            (<i>Sparse labels have been zoomed-in <b>4</b> times for the ease of visualisation.</i>)
        </p>


        <div class="row justify-content-left">
            <div class="col-sm-12">
                <div class="col justify-content-center text-center">
                    <!-- <h5>Super-resolution from two types of  labels</h5> -->
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_intro.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>Replica Office_3 Coarse</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/super-resolution/Super-Resolution_white_sp8_dense_office3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Office_3 Sparse</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/super-resolution/Super-Resolution_white_sp8_sparse_office3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        </br>

        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>ScanNet Scene0088_00 Coarse</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_sp8_dense_oscene0088.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>ScanNet Scene0088_00 Sparse</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_sp8_sparse_oscene0088.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Label Propagation (Interactive Segmentation)</h2>
        <hr>
        <p>
            Inspired by the success of label super-resolution, practical interactive annotation from users in form of clicks, strokes or scratches are desirable.
            We show that these partial labels can also be propagated to dense scene labelling by Semantic-NeRF.

            <br>
            Even single click per class/frame leads to very competitive semantic rendering of the whole scene.
            (<i>Single click labels below have been zoomed-in <b>9</b> times for the ease of visualisation.</i>)
        </p>


        <div class="row justify-content-left">
            <div class="col-sm-12">
                <div class="col justify-content-center text-center">
                    <h5>Replica Room_0 Single-Click</h5>
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/partial/Partial-1click_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>Replica Room_0 1% Label</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/partial/Partial-1perc_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Room_0 5% Label</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/partial/Partial-5perc_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        </br>
    </div>



    <div class="section">
        <h2>Semantic 3D Reconstruction from Posed Images</h2>
        <hr>
        <p>
            Explicit 3D meshes can be extracted from Semantic-NeRF by querying the MLP on dense grids within the scene, 
            and then applying marching cubes.
            Attached semantic texture is rendered by treating the <b>negative normal direction</b> of mesh vertices as the ray marching direction during volume rendering.
            
            Note that Semantic-NeRF is able to predict decent geometry and semantics even in occluded regions (e.g., areas behind the sofa) and  
            fill the holes to some extent in unobserved regions.
        </p>
                
        </br>

        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-12 padding-0 canvas-row">
                    <h6>Reconstructed mesh of Replica Room_0 with 256<sup>3</sup> grids. </h6>
                    <model-viewer
                            max-camera-orbit="Infinity 157.5deg 30m"
                            orientation="0deg -60deg 0deg"
                            camera-orbit="0deg 75deg 17m"
                            alt="Semantic Reconstruction from Posed Images"
                            src="img/sem_room0.glb"
                            poster="img/sem_room0_poster.png"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            auto-rotate
                            camera-controls>
                    </model-viewer>

                    <!-- src="img/sem_room0.glb" -->
                </div>
            </div>
        </div>

        <!-- Loads <model-viewer> for modern browsers: -->
        <script type="module"
                src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
        </script>
        <!-- <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script> -->
        <!-- <script type="module" src="./model-viewer.js"></script> -->
    </div>

    <div class="section">
        <h2>Conclusion</h2>
        <hr>
        <p>
            We have shown that adding a semantic output to a scene-specific implicit MLP model of geometry and appearance
            means that complete and high resolution semantic labels can be generated for a scene when only partial, noisy or 
            low-resolution semantic supervision is available. This method has practical uses in robotics or other applications 
            where scene understanding is required in new scenes where only limited labelling is possible.
            
            <br>

            An interesting direction for future research is interactive labelling, where the continually 
            training network asks for the new labels which will most resolve semantic ambiguity for the whole scene.
        </p>

    </div>


    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2103.15875"
                   class="list-group-item">
                    <img src="img/paper_overview.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{Ying:etal:ICCV2023,
                author = {Haiyang Ying and Baowei Jiang and Jinzhi Zhang and Di Xu and Tao Yu and Qionghai Dai and Lu Fang},
                title = {PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis},
                booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
                year={2023}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Feel free to send any feedback and questions to <a href="http://shuaifengzhi.com">Shuaifeng Zhi</a></p>
    </footer>
    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://vsitzmann.github.io/siren/">SIREN</a></small></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
