<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis">
    <meta name="author" content="Haiyang Ying,
                                Baowei Jiang,
                                Jinzhi Zhang,
                                Di Xu,
                                Tao Yu,
                                Qionghai Dai,
                                Lu Fang">

    <title>PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis</h2>
    <h3>(ICCV 2023)</h3>
    <hr>
    <p class="authors">
        <a href="https://oceanying.github.io/" target="_blank"> Haiyang Ying</a>,
        <a target="_blank"> Baowei Jiang</a>,
        <a target="_blank"> Jinzhi Zhang</a>,
        <a target="_blank"> Di Xu</a>,
        <a href="https://ytrock.com/" target="_blank"> Tao Yu</a>,
        <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html" target="_blank"> Qionghai Dai</a>,
        <a href="http://luvision.net/" target="_blank"> Lu Fang</a>,
    </p>

    <p class="authors">
       <a href="http://www.luvision.net/" target="_blank"> <b>Tsinghua University</b> </a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://oceanying.github.io/PARF/" target="_blank"> Paper </a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://oceanying.github.io/PARF/" target="_blank"> Video </a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/THU-luvision/PARF" target="_blank"> Code (Coming Soon) </a>
    </div>
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
<!--             We present PARF, Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis, which enables both efficient and high-quality rendering in general indoor scenes. 
            Standard volume based rendering methods like NeRF can model complex scenes but suffer from heavy sampling and ambiguous geometry.
            Primitive based rendering methods like NeurMips enjoys fast rendering but have difficulty representing complex geometric. -->

            We propose a method for fast scene radiance field reconstruction with strong novel view synthesis performance and convenient scene editing functionality. 
            The key idea is to fully utilize semantic parsing and primitive extraction for constraining and accelerating the radiance field reconstruction process. 
            To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. 
            We further contribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. 
            Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method.
            </br>
        </p>
        <img src="img/Teaser_v6.png" title="Performance comparison with the state-of-the-art radiance field reconstruction methods" class="center" width="100%">
        <hr>
    </div>

    <div class="section">
        <h2>Representation</h2>
        <hr>
        <p>
            Standard volume based rendering methods like NeRF can model complex scenes but suffer from heavy sampling and ambiguous geometry.
            Primitive based rendering methods like NeurMips enjoys fast rendering but have difficulty representing complex geometric.
            </br>
            We propose a novel hybrid representation to take advantage of both kinds of methods.
            Specifically, we represent the scene with a semantic volume, which consists three kinds of voxels with different sampling strategy. 
            D-voxels we simply use dense sampling like nerf, while for P-voxels we apply sparse primitive aware sampling strategy. 
            Besides, we simply skip sampling within E-voxels.
            </br>
        </p>
        <img src="img/figure2_final.png" title="Representation" class="center" width="90%">
        <hr>
    </div>
    
    <div class="section">
        <h2>Framework</h2>
        <hr>
        <p>
            we show the optimization framework for incremental radiance reconstruction.
            Given each RGBD image as input, we first detect primitives such as planes.
            Then we merge the detected new primitives into a global primitive list.
            Next, we back project the semantic frame into a 3D semantic volume to update the global semantic state.
            After that, primitive aware hybrid rendering is applied to render RGB values, depth values as well as semantic values which are supervised by the input frames.
            </br>
        </p>
        <img src="img/figure3_final.png" title="Framework" class="center" width="100%">
        <hr>
        <p>
            </br>
        </p>
    </div>

    <div class="section">
        <h2>Incremental Performance</h2>
        <hr>
        <p>
            We provide performance comparison between PARF and NeRF-SLAM which is in depth supervised version of InstantNGP.
            Notice our PARF enjoys much faster convergence with the help of primitive-aware hybrid representation.
            </br>
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-10">
                <h5>1. Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
            <hr>
            <div class="col-sm-10">
                <h5>2. Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm2.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
            <hr>
            <div class="col-sm-10">
                <h5>3. Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <hr>
        <p>
            </br>
        </p>
    </div>

    <div class="section">
        <h2>Extropolation Performance</h2>
        <hr>
        <p>
            We also show the extrapolation ability of our method. 
            Note that under extrapolation views, PARF shows robust rendering results with the help of primitive-aware representation, 
            while NeRF-SLAM shows blurry rendering results due to the ambiguous geometry.
            </br></br>
        </p>
        
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>1. Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>2. Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        </br>
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>3. Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>4. BundleFusion apt0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra4.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <hr>
        <p>
            </br>
        </p>
    </div>

        
    <div class="section">
        <h2>Real-time Interaction and Rendering</h2>
        <hr>
        <p>
            We show that our method is capable of real-time rendering and interactions.
            </br></br>
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-12">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
            <hr>
            <div class="col-sm-12">
                <h5>Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <hr>
        <p>
            </br>
        </p>
    </div>


    <div class="section">
        <h2>Sparse Reconstruction</h2>
        <hr>
        <p>
            Given only sparse view as input, PARF shows robust rendering performance thanks to the primitive based hybrid representation.
            </br>
        </p>
        <img src="img/sparsity.png" title="Representation" class="center" width="80%">
        <hr>
        <p>
            </br>
        </p>
    </div>

    
    <div class="section">
        <h2>Scene Editing</h2>
        <hr>
        <p>
            Our primitive-aware hybrid representation also enables convenient scene editing.
            </br>
        </p>
        <img src="img/Editing.png" title="Representation" class="center" width="80%">
        <hr>
        <p>
            </br>
        </p>
    </div>

    

    <div class="section">
        <h2>Conclusion</h2>
        <hr>
        <p>
            We introduce PARF, a Primitive-Aware Radiance Fusion method for indoor scene radiance field reconstruction and editing. 
            By combining volumetric and primitive rendering in a hybrid neural representation, 
            we successfully merge semantic parsing, primitive extraction, and radiance fusion into a single framework. 
            PARF achieves significant improvement in convergence speed, strong view extrapolation performance, and realistic semantic editing effects simultaneously. 
            Since the discrete semantic volume may lead to jagged primitive boundaries for novel view synthesis, 
            future work includes combining the semantic information in a more compact manner and adding more kinds of primitives for more effective reconstruction.
        </p>
        <hr>

    </div>

    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{Ying:etal:ICCV2023,
                author = {Haiyang Ying and Baowei Jiang and Jinzhi Zhang and Di Xu and Tao Yu and Qionghai Dai and Lu Fang},
                title = {PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis},
                booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
                year={2023}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Feel free to send any feedback and questions to <a href="https://oceanying.github.io/">Haiyang Ying</a></p>
    </footer>
    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://vsitzmann.github.io/siren/">SIREN</a></small></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
