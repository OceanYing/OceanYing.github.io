<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="In-Place Scene Labelling and Understanding with Implicit Scene Representation">
    <meta name="author" content="Shuaifeng Zhi,
                                Tristan Laidlow,
                                Stefan Leutenegger,
                                Andrew Davison">

    <title>In-Place Scene Labelling and Understanding with Implicit Scene Representation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>In-Place Scene Labelling and Understanding with Implicit Scene Representation</h2>
    <h3>ICCV 2021 (Oral)</h3>
    <hr>
    <p class="authors">
        <a href="http://shuaifengzhi.com/" target="_blank"> Shuaifeng Zhi</a>,
        <a href="https://wp.doc.ic.ac.uk/twl15/" target="_blank"> Tristan Laidlow</a>,
        <a href="https://wp.doc.ic.ac.uk/sleutene/" target="_blank"> Stefan Leutenegger</a>,
        <a href="https://www.doc.ic.ac.uk/~ajd/" target="_blank"> Andrew Davison</a>
    </p>

    <p class="authors">
       <small><a href="https://www.imperial.ac.uk/dyson-robotics-lab" target="_blank"> Dyson Robotics Laboratory, Imperial College London </a> </small>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2103.15875" target="_blank"> Paper (Arxiv) </a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://youtu.be/FpShWO7LVbM" target="_blank"> Video (Youtube) </a>
        <!-- <a class="btn btn-primary" href="https://www.bilibili.com/video/BV1FK4y1M7PC/"> Video (bilibili) </a> -->
    </div>
    <!-- <div class="btn-group btn-group-sm" role="group" aria-label="Top menu"> -->
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://www.bilibili.com/video/BV1FK4y1M7PC/" target="_blank"> Video (bilibili) </a>
    </div>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/Harry-Zhi/semantic_nerf/" target="_blank"> Code </a>
    </div>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://www.dropbox.com/sh/9yu1elddll00sdl/AAC-rSJdLX0C6HhKXGKMOIija?dl=0" target="_blank"> Data </a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <!-- <video width="100%" playsinline="" controls="" preload="" muted="">
                <source src="https://www.dropbox.com/s/h12sv0zlt5beqm8/Semantic_NeRF_Final_public.mp4?dl=1" type="video/mp4">
            </video> -->
            <iframe class='video' src="https://www.youtube.com/embed/FpShWO7LVbM" frameborder="0" 
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" 
            allowfullscreen></iframe>
            <!-- <source src="Semantic_NeRF_Final_public.mp4" type="video/mp4"> -->
        </div>
        <hr>
        <p>
            Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. 
            Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, 
            but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties.

            </br>
            </br>
            We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. 
            The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. 
            We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. 
            We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, 
            label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.
        </p>
    </div>

    <div class="section">
        <h2>Applications</h2>
        <hr>
        <p>
            Scene-specific implicit 3D semantic representation is obtained by training on colour images and semantic labels with associated poses. 
            Motivated by the enforcement of multi-view consistency and internal coherence within representation,
            <b>we find that the training process of Semantic-NeRF itself is a multi-view label fusion and propagation process, i.e., fusion via learning</b>.
            So that it is possible to train Semantic-NeRF efficiently to accurately render the full scene with various types of sparse or imperfect labels.
            </br>
            </br>
            We aim to demonstrate the benefits and promising applications of efficiently learning such a joint 3D representation for semantic labelling and understanding. 
    </div>


    <div class="section">
        <h2>Semantic View Synthesis</h2>
        <hr>
        <p>
            Our proposed semantic representation can be trained with labels from only few key-frames. 
            Pixels with high entropy matches well to object boundaries and uncertain/unknown regions.
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-12">
                <h5>Replica Room_2 (Using 20% Label of Full Sequence)</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <!-- <source src="https://www.dropbox.com/s/78h8sunsq6zrvg0/VS_room2.mp4?dl=1" type="video/mp4"> -->
                        <!-- <source src="https://www.dropbox.com/s/78h8sunsq6zrvg0/VS_room2.mp4?raw=1" type="video/mp4"> -->
                        <source src="img/VS_room2.mp4" type="video/mp4">
                    </video>
                    <!-- <source src="https://www.dropbox.com/s/78h8sunsq6zrvg0/VS_room2.mp4?raw=1" type="video/mp4"> -->
                    <!-- <source src="img/VS_room2.mp4" type="video/mp4"> -->
                </div>
            </div>
        </br>
    </div>


    <div class="section">
        <h2>Semantic Label Denoising</h2>
        <hr>
        <p>
            Semantic-NeRF, trained only with severely corrupted labels, is able to learn a smooth representation and render denoised input labels.

            While pixel-wise denoising at this level is not a realistic application, however, it is a very challenging task and highlights our key observation
            that training Semantic-NeRF itself is a fusion process which enables smooth renderings benefiting from the internal consistency and coherence of implicit joint representation.
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/denoise/denoise_office_0.mp4" type="video/mp4">
                        <!-- <source src="https://www.dropbox.com/s/vupdpk7i7e67iol/denoise_office_0.mp4?dl=1" type="video/mp4"> -->
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Room_1</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/denoise/denoise_room_1.mp4" type="video/mp4">
                        <!-- <source src="https://www.dropbox.com/s/efenn6vffvf5sls/denoise_room_1.mp4?dl=1" type="video/mp4"> -->
                    </video>
                </div>
            </div>
        </div>

        </br>
        
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>ScanNet Scene0010_00</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/denoise/denoise_scene0010_00.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>ScanNet Scene0012_00</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/denoise/denoise_scene0012_00.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Semantic Label Super-Resolution</h2>
        <hr>
        <p>
            Low-resolution labelling from light-weight CNNs or manual annotations are less costly to acquire than high-resolution ones. 
            By supervising Semantic-NeRF with only low-resolution labels, we can accurately super-resolve input labels.

            <br>
            Note that either coarse or sparse training labels use the same amount of information from low-resolution label maps.
            (<i>Sparse labels have been zoomed-in <b>4</b> times for the ease of visualisation.</i>)
        </p>


        <div class="row justify-content-left">
            <div class="col-sm-12">
                <div class="col justify-content-center text-center">
                    <!-- <h5>Super-resolution from two types of  labels</h5> -->
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_intro.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>Replica Office_3 Coarse</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/super-resolution/Super-Resolution_white_sp8_dense_office3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Office_3 Sparse</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/super-resolution/Super-Resolution_white_sp8_sparse_office3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        </br>

        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>ScanNet Scene0088_00 Coarse</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_sp8_dense_oscene0088.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>ScanNet Scene0088_00 Sparse</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/super-resolution/Super-Resolution_white_sp8_sparse_oscene0088.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Label Propagation (Interactive Segmentation)</h2>
        <hr>
        <p>
            Inspired by the success of label super-resolution, practical interactive annotation from users in form of clicks, strokes or scratches are desirable.
            We show that these partial labels can also be propagated to dense scene labelling by Semantic-NeRF.

            <br>
            Even single click per class/frame leads to very competitive semantic rendering of the whole scene.
            (<i>Single click labels below have been zoomed-in <b>9</b> times for the ease of visualisation.</i>)
        </p>


        <div class="row justify-content-left">
            <div class="col-sm-12">
                <div class="col justify-content-center text-center">
                    <h5>Replica Room_0 Single-Click</h5>
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/partial/Partial-1click_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        
        <div class="row justify-content-left">
            <div class="col-sm-6">
                <h5>Replica Room_0 1% Label</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/partial/Partial-1perc_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>Replica Room_0 5% Label</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                        <source src="img/partial/Partial-5perc_room0.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        </br>
    </div>



    <div class="section">
        <h2>Semantic 3D Reconstruction from Posed Images</h2>
        <hr>
        <p>
            Explicit 3D meshes can be extracted from Semantic-NeRF by querying the MLP on dense grids within the scene, 
            and then applying marching cubes.
            Attached semantic texture is rendered by treating the <b>negative normal direction</b> of mesh vertices as the ray marching direction during volume rendering.
            
            Note that Semantic-NeRF is able to predict decent geometry and semantics even in occluded regions (e.g., areas behind the sofa) and  
            fill the holes to some extent in unobserved regions.
        </p>
                
        </br>

        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-12 padding-0 canvas-row">
                    <h6>Reconstructed mesh of Replica Room_0 with 256<sup>3</sup> grids. </h6>
                    <model-viewer
                            max-camera-orbit="Infinity 157.5deg 30m"
                            orientation="0deg -60deg 0deg"
                            camera-orbit="0deg 75deg 17m"
                            alt="Semantic Reconstruction from Posed Images"
                            src="img/sem_room0.glb"
                            poster="img/sem_room0_poster.png"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            auto-rotate
                            camera-controls>
                    </model-viewer>

                    <!-- src="img/sem_room0.glb" -->
                </div>
            </div>
        </div>

        <!-- Loads <model-viewer> for modern browsers: -->
        <script type="module"
                src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
        </script>
        <!-- <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script> -->
        <!-- <script type="module" src="./model-viewer.js"></script> -->
    </div>

    <div class="section">
        <h2>Conclusion</h2>
        <hr>
        <p>
            We have shown that adding a semantic output to a scene-specific implicit MLP model of geometry and appearance
            means that complete and high resolution semantic labels can be generated for a scene when only partial, noisy or 
            low-resolution semantic supervision is available. This method has practical uses in robotics or other applications 
            where scene understanding is required in new scenes where only limited labelling is possible.
            
            <br>

            An interesting direction for future research is interactive labelling, where the continually 
            training network asks for the new labels which will most resolve semantic ambiguity for the whole scene.
        </p>

    </div>


    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2103.15875"
                   class="list-group-item">
                    <img src="img/paper_overview.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{Zhi:etal:ICCV2021,
                author = {Shuaifeng Zhi and Tristan Laidlow and Stefan Leutenegger and Andrew Davison},
                title = {In-Place Scene Labelling and Understanding with Implicit Scene Representation},
                booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
                year={2021}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Feel free to send any feedback and questions to <a href="http://shuaifengzhi.com">Shuaifeng Zhi</a></p>
    </footer>
    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://vsitzmann.github.io/siren/">SIREN</a></small></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
