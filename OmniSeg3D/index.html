<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning">
    <meta name="author" content="Haiyang Ying,
                                Yixuan Yin,
                                Jinzhi Zhang,
                                Fan Wang,
                                Tao Yu,
                                Ruqi Huang,
                                Lu Fang">

    <title>OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</h2>
    <h3>(ArXiv 2023)</h3>
    <hr>
    <p class="authors">
        <a href="https://oceanying.github.io/" target="_blank"> Haiyang Ying</a>,
        <a target="_blank"> Yixuan Yin</a>,
        <a target="_blank"> Jinzhi Zhang</a>,
        <a target="_blank"> Fan Wang</a>,
        <a href="https://ytrock.com/" target="_blank"> Tao Yu</a>,
        <a href="https://rqhuang88.github.io/" target="_blank"> Ruqi Huang</a>,
        <a href="http://luvision.net/" target="_blank"> Lu Fang</a>,
    </p>

    <p class="authors">
<!--        <a href="http://www.luvision.net/" target="_blank"> <b>Tsinghua University</b> </a> -->
       <a> <b>Tsinghua University</b> </a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://oceanying.github.io/PARF/" target="_blank"> Paper </a>
    </div>
<!--     <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://oceanying.github.io/PARF/" target="_blank"> Video </a>
    </div> -->
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/THU-luvision/PARF" target="_blank"> Code (Coming Soon) </a>
    </div>
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Towards holistic understanding of 3D scenes, a general 3D segmentation method is needed 
            that can segment diverse objects without restrictions on object quantity or categories, 
            while also reflecting the inherent hierarchical structure. To achieve this, we propose OmniSeg3D, 
            an omniversal segmentation method aims for segmenting anything in 3D all at once. 
            The key insight is to lift multi-view inconsistent 2D segmentations into a consistent 3D feature field 
            through a hierarchical contrastive learning framework, which is accomplished by two steps. 
            Firstly, we design a novel hierarchical representation based on category-agnostic 2D segmentations 
            to model the multi-level relationship among pixels. Secondly, image features rendered from the 3D feature field
            are clustered at different levels, which can be further drawn closer or pushed apart according to 
            the hierarchical relationship between different levels. In tackling the challenges posed by inconsistent 2D segmentations, 
            this framework yields a global consistent 3D feature field, which further enables hierarchical segmentation, 
            multi-object selection, and global discretization. Extensive experiments demonstrate the effectiveness of our method
            on high-quality 3D segmentation and accurate hierarchical structure understanding. 
            A graphical user interface further facilitates flexible interaction for omniversal 3D segmentation.
            </br>
        </p>
        <img src="img/Teaser_v6.png" title="Performance comparison with the state-of-the-art radiance field reconstruction methods" class="center" width="100%">
        <hr>
    </div>

<!--     <div class="section">
        <h2>Representation</h2>
        <hr>
        <p>
            Standard volume based rendering methods like NeRF can model complex scenes but suffer from heavy sampling and ambiguous geometry.
            Primitive based rendering methods like NeurMips enjoys fast rendering but have difficulty representing complex geometric.
            </br>
            We propose a novel hybrid representation to take advantage of both kinds of methods.
            Specifically, we represent the scene with a semantic volume, which consists three kinds of voxels with different sampling strategy. 
            D-voxels we simply use dense sampling like nerf, while for P-voxels we apply sparse primitive aware sampling strategy. 
            Besides, we simply skip sampling within E-voxels.
            </br>
        </p>
        <img src="img/figure2_final.png" title="Representation" class="center" width="90%">
        <hr>
    </div> -->
    
<!--     <div class="section">
        <h2>Framework</h2>
        <hr>
        <p>
            we show the optimization framework for incremental radiance reconstruction.
            Given each RGBD image as input, we first detect primitives such as planes.
            Then we merge the detected new primitives into a global primitive list.
            Next, we back project the semantic frame into a 3D semantic volume to update the global semantic state.
            After that, primitive aware hybrid rendering is applied to render RGB values, depth values as well as semantic values which are supervised by the input frames.
            </br>
        </p>
        <img src="img/figure3_final.png" title="Framework" class="center" width="100%">
        <hr>
        <p>
            </br>
        </p>
    </div> -->

    <div class="section">
        <h2>Interactive Multi-object Segmentation</h2>
        <hr>
        <p>
            We show multi-object segmentation performance on a challenging scene "Tengwang Pavilion", 
            which contains lots of object and parts that has ambiguious definition. Our method shows robust performance 
            on category-agnostic object/part segmentation.
            </br>
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-10">
                <h5>1. Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/twg_interactive_demo_comp.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
<!--             <hr>
            <div class="col-sm-10">
                <h5>2. Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm2.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
            <hr> -->
<!--             <div class="col-sm-10">
                <h5>3. Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_increm3.mp4" type="video/mp4">
                    </video>
                </div>
            </div> -->
        </div>
        <hr>
        <p>
            </br>
        </p>
    </div>

    <div class="section">
        <h2>Global Discretization Performance</h2>
        <hr>
        <p>
            We also show the 3D discretization ability of our method. 
            </br></br>
        </p>
        
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>1. Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
<!--             <div class="col-sm-6">
                <h5>2. Replica Office_2</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra2.mp4" type="video/mp4">
                    </video>
                </div>
            </div> -->
        </div>
<!--         </br>
        <div class="row justify-content-left">
        <!-- <div class="row align-items-center"> -->
            <div class="col-sm-6">
                <h5>3. Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra3.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
            <div class="col-sm-6">
                <h5>4. BundleFusion apt0</h5>
                <div class="col justify-content-center text-center">
                    <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted=""> -->
                    <video width="100%" playsinline="" controls="" loop="" preload="" muted="">
                        <source src="img/demo5_video_extra4.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div> -->
        <hr>
        <p>
            </br>
        </p>
    </div>

        
    <div class="section">
        <h2>Real-time Interaction and Rendering</h2>
        <hr>
        <p>
            We show that our method is capable of real-time rendering and interactions.
            </br></br>
        </p>
        <!-- <div class="row align-items-center"> -->
        <div class="row justify-content-left">
            <div class="col-sm-12">
                <h5>Replica Office_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>
                    </br>
                </p>
            </div>
            <hr>
            <div class="col-sm-12">
                <h5>Replica Room_0</h5>
                <div class="col justify-content-center text-center">
                    <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/demo5_video_drag2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
        <hr>
        <p>
            </br>
        </p>
    </div>


    <div class="section">
        <h2>Sparse Reconstruction</h2>
        <hr>
        <p>
            Given only sparse view as input, PARF shows robust rendering performance thanks to the primitive based hybrid representation.
            </br>
        </p>
        <img src="img/sparsity.png" title="Representation" class="center" width="80%">
        <hr>
        <p>
            </br>
        </p>
    </div>

    
    <div class="section">
        <h2>Scene Editing</h2>
        <hr>
        <p>
            Our primitive-aware hybrid representation also enables convenient scene editing.
            </br>
        </p>
        <img src="img/Editing.png" title="Representation" class="center" width="80%">
        <hr>
        <p>
            </br>
        </p>
    </div>

    

    <div class="section">
        <h2>Conclusion</h2>
        <hr>
        <p>
            We introduce PARF, a Primitive-Aware Radiance Fusion method for indoor scene radiance field reconstruction and editing. 
            By combining volumetric and primitive rendering in a hybrid neural representation, 
            we successfully merge semantic parsing, primitive extraction, and radiance fusion into a single framework. 
            PARF achieves significant improvement in convergence speed, strong view extrapolation performance, and realistic semantic editing effects simultaneously. 
            Since the discrete semantic volume may lead to jagged primitive boundaries for novel view synthesis, 
            future work includes combining the semantic information in a more compact manner and adding more kinds of primitives for more effective reconstruction.
        </p>
        <hr>

    </div>

    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{Ying:etal:ICCV2023,
                author = {Haiyang Ying and Baowei Jiang and Jinzhi Zhang and Di Xu and Tao Yu 
                          and Qionghai Dai and Lu Fang},
                title = {PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis},
                booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
                year={2023}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Feel free to send any feedback and questions to <a href="https://oceanying.github.io/">Haiyang Ying</a></p>
    </footer>
    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://vsitzmann.github.io/siren/">SIREN</a></small></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
